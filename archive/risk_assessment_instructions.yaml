version: 1
name: incident-risk-assessment-and-sanity-check
description: >
  Add incident + risk assessment models and agents (preprocessor, risk assessor,
  judge) plus a general text sanity-check module that can be used whenever text
  is ingested (legal texts, AI incidents, etc.) in the trustworthy_project_v0.

context:
  project_root: trustworthy_project_v0
  existing_files:
    - .env
    - eu_ai_act.txt
    - config/neo4j_config.py
    - models/legal_structure.py
    - models/legal_preprocess.py
    - models/legal_chunks.py
    - eu_ai_act_splitter.py
    - agent_preprocess_eu_ai_act.py
    - ingest_preprocessed_legal.py
    - run_preprocess_eu_ai_act.py
  data_files:
    - path: eu_ai_act.txt
      url: /mnt/data/eu_ai_act.txt
      description: >
        Full plain-text of Regulation (EU) 2024/1689 – the EU AI Act. Used for
        legal ingestion and sanity checks.

goals:
  - Add Pydantic models for:
      - AI incidents (based largely on AI Incident Database fields),
      - risk assessments under the EU AI Act,
      - risk assessment judgements.
  - Add a general text sanity-check module to compute basic stats and be usable
    for any ingested text (legal documents, incidents).
  - Implement an incident preprocessor agent that:
      - takes AI Incident Database-style fields,
      - fills an AIIncident model,
      - is conservative (leaves optional derived fields as None when unsure).
  - Implement deterministic Neo4j ingestion for AIIncident nodes.
  - Implement a risk assessor agent that:
      - takes an AIIncident and a list of candidate EU AI Act provisions from Neo4j,
      - outputs a RiskAssessment grounded ONLY in those provisions.
  - Implement a judge agent that critiques RiskAssessment and outputs a
    RiskAssessmentJudgement.
  - Provide two scripts:
      - run_incident_preprocess_aiid.py: ingest AIID-style incidents into Neo4j,
      - run_assess_incident.py: given an incident_id, run risk assessment + judge.

non_goals:
  - No complex or highly-optimized Neo4j queries for provisions yet; start with
    simple, explainable heuristics (e.g., keyword-based).
  - No UI beyond simple CLI scripts.
  - No multi-database support; start with AI Incident Database as primary source.

tasks:

  - id: models-incidents
    description: >
      Create Pydantic models for AI incidents, risk assessment, and risk judgement.
    changes:
      - path: models/incidents.py
        purpose: >
          Define SourceInfo, AIIncident, ProvisionRef, PotentialViolation,
          RequiredObligation, RiskAssessment, RiskAssessmentJudgement.
        spec: |
          Create models/incidents.py with:

          - Imports:
              - from typing import Optional, List, Literal
              - from pydantic import BaseModel, Field, HttpUrl

          - Define SourceInfo:

            class SourceInfo(BaseModel):
                """
                Reference to an external incident source (e.g. AI Incident Database).
                """
                source_db: str = Field(
                    description="Name of the database, e.g. 'AI Incident Database'."
                )
                source_id: str = Field(
                    description="ID in that database, e.g. '4' or 'AIID-2016-0004'."
                )
                source_url: Optional[HttpUrl] = Field(
                    default=None,
                    description="Direct URL to the incident entry, if available."
                )

          - Define AIIncident:

            class AIIncident(BaseModel):
                """
                Structured incident from AI Incident Database (or similar).

                Some fields map directly to the table-style source; others are
                derived by the preprocessor agent.
                """
                incident_id: str = Field(
                    description="Internal ID in this project, e.g. 'incident_0004'."
                )
                source: SourceInfo = Field(
                    description="External database reference."
                )

                # Direct fields from AI Incident Database-style input
                title: str = Field(
                    description="Short title of the incident."
                )
                description: str = Field(
                    description="Full incident description from the source."
                )
                date: Optional[str] = Field(
                    default=None,
                    description="Incident or report date in YYYY-MM-DD form if possible."
                )
                alleged_deployer: Optional[str] = Field(
                    default=None,
                    description="Alleged deployer of the AI system."
                )
                alleged_developer: Optional[str] = Field(
                    default=None,
                    description="Alleged developer of the AI system."
                )
                alleged_harmed_parties: Optional[str] = Field(
                    default=None,
                    description="Alleged harmed or nearly harmed parties."
                )
                implicated_systems: Optional[str] = Field(
                    default=None,
                    description="Names of implicated AI systems, if available."
                )

                # Derived / normalized semantics (optional, filled by LLM)
                system_function: Optional[str] = Field(
                    default=None,
                    description="What the AI system does (e.g. 'autonomous driving', 'content recommendation')."
                )
                deployment_context: Optional[str] = Field(
                    default=None,
                    description="Sector/use case (e.g. 'transport', 'healthcare', 'social media')."
                )
                affected_individuals: Optional[str] = Field(
                    default=None,
                    description="Who is affected (e.g. 'children', 'pedestrians', 'employees')."
                )
                harms_observed: Optional[str] = Field(
                    default=None,
                    description="Short summary of harms/risks (e.g. 'physical injury', 'discrimination')."
                )

          - Define risk-related models:

            RiskLevel = Literal["UNACCEPTABLE", "HIGH", "LIMITED", "MINIMAL", "UNCERTAIN"]

            class ProvisionRef(BaseModel):
                """
                Reference to a specific EU AI Act provision.
                """
                article: int = Field(description="Article number, e.g. 5.")
                paragraph: Optional[int] = Field(
                    default=None, description="Paragraph number, if applicable."
                )
                point: Optional[str] = Field(
                    default=None, description="Lettered point marker, e.g. 'a', if applicable."
                )
                node_id: Optional[str] = Field(
                    default=None,
                    description="Optional Neo4j node id of the referenced provision."
                )

            class PotentialViolation(BaseModel):
                provision: ProvisionRef
                rationale: str = Field(
                    description="Explanation of how this incident may violate this provision."
                )

            class RequiredObligation(BaseModel):
                provision: ProvisionRef
                obligation_summary: str = Field(
                    description="Summary of what the system should comply with under this provision."
                )

            class RiskAssessment(BaseModel):
                """
                Legal risk assessment of one incident under the EU AI Act.

                All important claims should reference explicit provisions.
                """
                incident_id: str = Field(
                    description="ID of the AIIncident this assessment refers to."
                )
                source: SourceInfo = Field(
                    description="Same source reference as the underlying incident."
                )
                assessed_text: str = Field(
                    description=(
                        "The exact text segment being assessed (usually the incident description)."
                    )
                )
                risk_level: RiskLevel = Field(
                    description="Overall risk level under the EU AI Act (or UNCERTAIN)."
                )
                potential_violations: List[PotentialViolation] = Field(
                    default_factory=list,
                    description="Provisions that may be violated, with rationales."
                )
                relevant_obligations: List[RequiredObligation] = Field(
                    default_factory=list,
                    description="Provisions describing obligations that should have been followed."
                )
                relevant_roles: List[str] = Field(
                    default_factory=list,
                    description="Likely roles involved (e.g. 'provider', 'deployer')."
                )
                overall_explanation: str = Field(
                    description="Narrative explanation connecting assessed_text, risk_level, and provisions."
                )

            class RiskAssessmentJudgement(BaseModel):
                """
                Judge agent's view on a RiskAssessment's consistency with the law text.
                """
                incident_id: str = Field(
                    description="ID of the incident this judgement relates to."
                )
                is_consistent: bool = Field(
                    description="Whether the assessment is consistent with the provided EU AI Act text."
                )
                score: float = Field(
                    description="Confidence score between 0 and 1."
                )
                issues: List[str] = Field(
                    default_factory=list,
                    description="List of detected issues or inconsistencies."
                )
                corrected_assessment: Optional[RiskAssessment] = Field(
                    default=None,
                    description="Optional corrected assessment if the judge proposes changes."
                )

  - id: text-sanity-module
    description: >
      Create a general text sanity-check module to compute basic stats for any
      ingested text (legal document, incident description, etc.) and integrate it
      into existing ingestion scripts.
    changes:
      - path: text_sanity.py
        purpose: >
          Provide generic text stats / fingerprint functions and optional Neo4j
          logging to support completeness and sanity checks.
        spec: |
          Create text_sanity.py with:

          - Imports:
              - from typing import Optional
              - from pydantic import BaseModel, Field
              - import hashlib

          - Define TextStats:

            class TextStats(BaseModel):
                """
                Basic sanity-check statistics for a text blob.

                Can be computed for any ingested text (legal documents, incidents, transcripts).
                """
                source_type: str = Field(
                    description="High-level type of text, e.g. 'EU_AI_ACT', 'AI_INCIDENT', 'TRANSCRIPT'."
                )
                source_id: str = Field(
                    description="Identifier for the text (e.g. 'eu_ai_act_2024', 'incident_0004')."
                )
                char_count: int = Field(
                    description="Number of characters in the text."
                )
                word_count: int = Field(
                    description="Number of whitespace-separated tokens."
                )
                line_count: int = Field(
                    description="Number of newline-separated lines."
                )
                sha256: str = Field(
                    description="SHA-256 hex digest of the text (for change detection)."
                )

          - Provide a function to compute stats:

            def compute_text_stats(text: str, source_type: str, source_id: str) -> TextStats:
                """
                Compute basic stats (chars, words, lines, hash) for any given text.

                Args:
                    text: The raw text.
                    source_type: A label like 'EU_AI_ACT' or 'AI_INCIDENT'.
                    source_id: Unique identifier for this text within that type.

                Returns:
                    TextStats instance.
                """
                char_count = len(text)
                line_count = text.count("\n") + 1 if text else 0
                word_count = len(text.split())
                sha256 = hashlib.sha256(text.encode("utf-8")).hexdigest()
                return TextStats(
                    source_type=source_type,
                    source_id=source_id,
                    char_count=char_count,
                    word_count=word_count,
                    line_count=line_count,
                    sha256=sha256,
                )

          - Provide a simple printer:

            def print_text_stats(stats: TextStats) -> None:
                """
                Print text stats in a human-readable form.

                Useful in ingestion scripts to quickly inspect sizes and hashes.
                """
                print(f"Text stats for {stats.source_type} / {stats.source_id}:")
                print(f"  Characters: {stats.char_count}")
                print(f"  Words:      {stats.word_count}")
                print(f"  Lines:      {stats.line_count}")
                print(f"  SHA-256:    {stats.sha256}")

      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Integrate compute_text_stats into legal ingestion so every run shows
          basic stats for the ingested legal text.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - Add import near the top:

            from text_sanity import compute_text_stats, print_text_stats

          - After reading raw_text in preprocess_eu_ai_act_by_chunks (or equivalent),
            compute and print stats:

            stats = compute_text_stats(raw_text, source_type="EU_AI_ACT", source_id=DOCUMENT_ID)
            print_text_stats(stats)

          This should happen before calling the chunk-level preprocessing agent.

  - id: incident-preprocess-agent
    description: >
      Implement an incident preprocessor agent that takes AIID-style fields and
      outputs an AIIncident Pydantic object.
    changes:
      - path: agent_incident_preprocess.py
        purpose: >
          Define IncidentPreprocessDeps and incident_preprocess_agent using the
          PydanticAI library and AIIncident model.
        spec: |
          Create agent_incident_preprocess.py with:

          - Imports:
              - from dotenv import load_dotenv
              - from pydantic_ai import Agent
              - from pydantic_ai.models.openai import OpenAIChatModel
              - from pydantic import BaseModel, Field
              - from models.incidents import AIIncident, SourceInfo

          - Load environment variables:

            load_dotenv()

          - Define IncidentPreprocessDeps:

            class IncidentPreprocessDeps(BaseModel):
                """
                Deterministic metadata for incident preprocessing.

                All values are provided by the caller and must be echoed back
                by the agent. The agent MUST NOT change them.
                """
                incident_id: str = Field(
                    description="Internal incident ID, e.g. 'incident_0004'."
                )
                source_db: str = Field(
                    description="Name of the incident database, e.g. 'AI Incident Database'."
                )
                source_id: str = Field(
                    description="ID in that database, e.g. '4'."
                )
                source_url: Optional[str] = Field(
                    default=None,
                    description="URL of the incident entry, if known."
                )

          - Configure model:

            model = OpenAIChatModel(
                model_name="gpt-4o-mini",
            )

          - Define incident_preprocess_agent:

            incident_preprocess_agent = Agent[IncidentPreprocessDeps, AIIncident](
                model,
                deps_type=IncidentPreprocessDeps,
                output_type=AIIncident,
                instructions=\"\"\"You are an incident preprocessing specialist.

            Your task is to take AI Incident Database-style fields and produce a
            structured AIIncident object.

            INPUT:
              - Deterministic metadata (deps):
                  - incident_id
                  - source_db
                  - source_id
                  - source_url
              - Prompt text containing fields:
                  - title
                  - description
                  - date
                  - alleged_deployer
                  - alleged_developer
                  - alleged_harmed_parties
                  - implicated_systems

            OUTPUT:
              - An AIIncident with:
                  - incident_id echoed EXACTLY from deps
                  - source populated from deps (source_db, source_id, source_url)
                  - title, description, date, alleged_* fields filled from input
                  - system_function, deployment_context, affected_individuals,
                    harms_observed filled conservatively from the description

            CRITICAL RULES:

            1. NO ID HALLUCINATION
               - Do NOT change incident_id, source_db, source_id, or source_url.
               - Echo them exactly in the AIIncident.source and incident_id fields.

            2. TEXT HANDLING
               - Preserve title and description text as given (you may normalize whitespace).
               - Do NOT invent extra incidents or merge multiple incidents.

            3. DERIVED FIELDS
               - system_function: describe what the AI system does (e.g. 'autonomous driving',
                 'content recommendation', 'NLP word embedding').
               - deployment_context: sector/use case (e.g. 'transport', 'healthcare',
                 'social media', 'criminal justice').
               - affected_individuals: who is affected (e.g. 'children', 'pedestrians',
                 'employees', 'patients').
               - harms_observed: summarize harms/risks (e.g. 'physical injury',
                 'discrimination', 'exposure to harmful content').
               - If you are uncertain, leave these fields as null (None) instead of guessing.

            4. COMPLETENESS & CONSERVATISM
               - It is better to leave a derived field null than to guess.
               - Do NOT invent new actors or systems.
               - If a field is missing in the input (e.g. no implicated_systems), keep it null.

            5. OUTPUT
               - Return only the AIIncident object, no extra commentary.
               - Ensure the output conforms strictly to the AIIncident schema.

            Your job is to create a clean, conservative structured incident representation,
            not to make legal or ethical judgements.
            \"\"\",
            )

  - id: ingest-incident-neo4j
    description: >
      Implement deterministic Neo4j ingestion for AIIncident objects.
    changes:
      - path: ingest_incident.py
        purpose: >
          Provide functions to write AIIncident to Neo4j as Incident nodes.
        spec: |
          Create ingest_incident.py with:

          - Imports:
              - from typing import Optional
              - from models.incidents import AIIncident
              - from config.neo4j_config import get_neo4j_driver

          - Implement:

            def ingest_incident(incident: AIIncident) -> None:
                """
                Deterministically write an AIIncident to Neo4j.

                Schema:

                  (i:Incident {
                      incident_id,
                      source_db,
                      source_id,
                      source_url,
                      title,
                      description,
                      date,
                      alleged_deployer,
                      alleged_developer,
                      alleged_harmed_parties,
                      implicated_systems,
                      system_function,
                      deployment_context,
                      affected_individuals,
                      harms_observed
                  })

                For now we do not create relationships to other nodes; those may
                be added later.
                """
                cypher = """
                MERGE (i:Incident {incident_id: $incident_id})
                SET i.source_db = $source_db,
                    i.source_id = $source_id,
                    i.source_url = $source_url,
                    i.title = $title,
                    i.description = $description,
                    i.date = $date,
                    i.alleged_deployer = $alleged_deployer,
                    i.alleged_developer = $alleged_developer,
                    i.alleged_harmed_parties = $alleged_harmed_parties,
                    i.implicated_systems = $implicated_systems,
                    i.system_function = $system_function,
                    i.deployment_context = $deployment_context,
                    i.affected_individuals = $affected_individuals,
                    i.harms_observed = $harms_observed
                """
                params = {
                    "incident_id": incident.incident_id,
                    "source_db": incident.source.source_db,
                    "source_id": incident.source.source_id,
                    "source_url": str(incident.source.source_url) if incident.source.source_url else None,
                    "title": incident.title,
                    "description": incident.description,
                    "date": incident.date,
                    "alleged_deployer": incident.alleged_deployer,
                    "alleged_developer": incident.alleged_developer,
                    "alleged_harmed_parties": incident.alleged_harmed_parties,
                    "implicated_systems": incident.implicated_systems,
                    "system_function": incident.system_function,
                    "deployment_context": incident.deployment_context,
                    "affected_individuals": incident.affected_individuals,
                    "harms_observed": incident.harms_observed,
                }
                with get_neo4j_driver() as driver:
                    driver.execute_query(cypher, **params)
                print(f"✓ Ingested incident {incident.incident_id} into Neo4j")

  - id: run-incident-preprocess-aiid
    description: >
      Add a script to preprocess AI Incident Database-style rows into AIIncident
      objects and ingest them into Neo4j, with text sanity checks.
    changes:
      - path: run_incident_preprocess_aiid.py
        purpose: >
          Provide a simple CLI script that takes a hardcoded sample or CSV row
          and runs the incident preprocessor agent + ingestion.
        spec: |
          Create run_incident_preprocess_aiid.py with:

          - Imports:
              - import asyncio
              - from models.incidents import AIIncident, SourceInfo
              - from agent_incident_preprocess import incident_preprocess_agent, IncidentPreprocessDeps
              - from ingest_incident import ingest_incident
              - from text_sanity import compute_text_stats, print_text_stats

          - For simplicity, start with a small hardcoded sample list of dicts that
            mimic AI Incident Database columns like the examples the user provided
            (Google YouTube Kids, Uber AV, TayBot, etc.). Use it as initial test
            data; optionally comment how to replace with CSV loading later.

          - Implement an async function preprocess_one_incident_row(row: dict, idx: int) -> AIIncident:
              - Build IncidentPreprocessDeps from idx and row["Incident ID"] (or similar).
              - Build a prompt string that clearly shows the raw fields:
                  - title
                  - description
                  - date
                  - alleged_deployer
                  - alleged_developer
                  - alleged_harmed_parties
                  - implicated_systems
              - Call incident_preprocess_agent.run(prompt, deps=deps).
              - Return result.output (AIIncident).

              - Before calling the agent, compute_text_stats on row["Description"] with
                source_type="AI_INCIDENT" and source_id=deps.incident_id, then print_text_stats.

          - Implement async main() that:
              - Iterates over the sample incidents (or future CSV rows).
              - Calls preprocess_one_incident_row for each.
              - Calls ingest_incident(incident) for each.
              - Prints summaries.

          - Add __main__ guard with asyncio.run(main()) and instructions in comments
            on how to replace the hardcoded list with CSV reading later.

  - id: risk-assessor-agent
    description: >
      Implement a risk assessor agent that takes AIIncident + candidate provisions
      and outputs a RiskAssessment grounded in those provisions.
    changes:
      - path: agent_risk_assessor.py
        purpose: >
          Define RiskAssessorDeps and risk_assessor_agent using PydanticAI and
          the RiskAssessment model.
        spec: |
          Create agent_risk_assessor.py with:

          - Imports:
              - from dotenv import load_dotenv
              - from pydantic_ai import Agent
              - from pydantic_ai.models.openai import OpenAIChatModel
              - from pydantic import BaseModel, Field
              - from typing import List, Tuple
              - from models.incidents import AIIncident, RiskAssessment, ProvisionRef
              - from models.incidents import SourceInfo

          - load_dotenv()

          - Define RiskAssessorDeps:

            class RiskAssessorDeps(BaseModel):
                """
                Deterministic metadata for risk assessment calls.
                """
                assessment_id: str = Field(
                    description="Internal ID for this assessment, e.g. 'assessment_incident_0004'."
                )

          - Configure model:

            model = OpenAIChatModel(
                model_name="gpt-4o-mini",
            )

          - Define risk_assessor_agent:

            risk_assessor_agent = Agent[RiskAssessorDeps, RiskAssessment](
                model,
                deps_type=RiskAssessorDeps,
                output_type=RiskAssessment,
                instructions=\"\"\"You are a risk assessment agent for the EU AI Act.

            You will receive:
              - An AIIncident object (structured description of an AI-related incident).
              - A list of candidate EU AI Act provisions, each with:
                  - article, paragraph, point (ProvisionRef)
                  - full text of the provision.

            Your task is to:
              - Assign an overall risk_level under the EU AI Act:
                  - UNACCEPTABLE, HIGH, LIMITED, MINIMAL, or UNCERTAIN.
              - For each potential violation, reference specific provisions (ProvisionRef)
                and explain why they may be violated.
              - Identify relevant obligations from provisions and summarize them.
              - Express all legal reasoning ONLY using the provided provisions.

            CRITICAL RULES:

            1. GROUNDING IN PROVIDED TEXT
               - You MUST base all legal claims exclusively on the provided provisions.
               - Do NOT cite or rely on provisions that are not in the input list.
               - If you need to mention a provision that is missing, explain this in
                 the overall_explanation, but still keep risk_level = 'UNCERTAIN'.

            2. RISK LEVEL
               - Choose the risk_level based on the EU AI Act framework as far as it
                 can be inferred from the provided provisions.
               - If you cannot justify a specific level with the given provisions,
                 choose 'UNCERTAIN'.

            3. STRUCTURE
               - incident_id and source in the RiskAssessment must match the input AIIncident.
               - assessed_text should usually be the AIIncident.description.
               - Each PotentialViolation.provision must correspond to one of the provided ProvisionRef entries.
               - Each RequiredObligation.provision must also correspond to a provided ProvisionRef.

            4. OUTPUT
               - Return a single RiskAssessment instance.
               - No extra commentary outside the structured output.

            Your job is to provide a conservative, provision-grounded assessment
            of risk, not to speculate beyond the provided law text.
            \"\"\",
            )

  - id: risk-judge-agent
    description: >
      Implement a judge agent that checks whether a RiskAssessment is consistent
      with the provided EU AI Act provisions.
    changes:
      - path: agent_risk_judge.py
        purpose: >
          Define RiskJudgeDeps and risk_judge_agent returning RiskAssessmentJudgement.
        spec: |
          Create agent_risk_judge.py with:

          - Imports:
              - from dotenv import load_dotenv
              - from pydantic_ai import Agent
              - from pydantic_ai.models.openai import OpenAIChatModel
              - from pydantic import BaseModel, Field
              - from typing import List, Tuple
              - from models.incidents import AIIncident, RiskAssessment, RiskAssessmentJudgement, ProvisionRef

          - load_dotenv()

          - Define RiskJudgeDeps:

            class RiskJudgeDeps(BaseModel):
                """
                Deterministic metadata for risk assessment judgement calls.
                """
                judgement_id: str = Field(
                    description="Internal ID for this judgement, e.g. 'judge_incident_0004'."
                )

          - Configure model:

            model = OpenAIChatModel(
                model_name="gpt-4o-mini",
            )

          - Define risk_judge_agent:

            risk_judge_agent = Agent[RiskJudgeDeps, RiskAssessmentJudgement](
                model,
                deps_type=RiskJudgeDeps,
                output_type=RiskAssessmentJudgement,
                instructions=\"\"\"You are a judge agent reviewing a RiskAssessment under the EU AI Act.

            You will receive:
              - An AIIncident object.
              - A list of candidate EU AI Act provisions (ProvisionRef + text).
              - A RiskAssessment produced by another agent.

            Your task is to:
              - Decide whether the RiskAssessment is consistent with the provided law text.
              - Identify any issues (misquoted provisions, unsupported risk level, missing provisions).
              - Optionally propose a corrected RiskAssessment if necessary.

            CRITICAL RULES:

            1. PROVISION CONSISTENCY
               - Check that all PotentialViolation.provision and RequiredObligation.provision
                 entries correspond to provisions in the provided list.
               - If the assessment cites a provision not in the list, flag this as an issue.

            2. TEXT CONSISTENCY
               - Compare the rationales and overall_explanation with the actual text of
                 the cited provisions. Flag any major misinterpretations.

            3. RISK LEVEL
               - Evaluate whether the chosen risk_level is plausible given the cited
                 provisions. If clearly misaligned, flag it and propose a corrected
                 risk_level in corrected_assessment.

            4. OUTPUT SCHEMA
               - is_consistent: True only if there are no major issues.
               - score: judge your confidence between 0 and 1.
               - issues: list human-readable explanations of problems.
               - corrected_assessment: provide a full RiskAssessment if you make changes.
               - incident_id must match the input incident.

            5. SCOPE
               - You are NOT adding new law text; you only judge consistency between
                 the assessment and the provided provisions.

            Your job is to act as a conservative checker to support trustworthy
            risk assessments.
            \"\"\",
            )

  - id: run-assess-incident
    description: >
      Add a CLI script that, given an incident_id, loads AIIncident from Neo4j,
      queries candidate provisions, calls risk_assessor_agent and risk_judge_agent,
      and prints/stores results.
    changes:
      - path: run_assess_incident.py
        purpose: >
          Provide an entrypoint for risk assessment based on an incident already
          stored in Neo4j.
        spec: |
          Create run_assess_incident.py with:

          - Imports:
              - import asyncio
              - from typing import List, Tuple
              - from config.neo4j_config import execute_query
              - from models.incidents import AIIncident, RiskAssessment, RiskAssessmentJudgement, ProvisionRef, SourceInfo
              - from agent_risk_assessor import risk_assessor_agent, RiskAssessorDeps
              - from agent_risk_judge import risk_judge_agent, RiskJudgeDeps

          - Implement helper to load AIIncident from Neo4j using incident_id.
            Reconstruct an AIIncident from node properties; if not found, raise.

          - Implement helper to query candidate EU AI Act provisions:
              - For now, keep it simple: query some Article nodes and return a small
                list of ProvisionRef + text (e.g., limit to 20).
              - Later you can refine this based on incident.deployment_context, etc.

          - Implement async assess_incident(incident_id: str):
              - Load AIIncident.
              - Query candidate provisions.
              - Build a prompt for risk_assessor_agent summarizing:
                  - the AIIncident (convert to JSON-like representation or plain text),
                  - the list of provisions with numbers + text.
              - Call risk_assessor_agent.run(prompt, deps=RiskAssessorDeps(...)).
              - Get RiskAssessment.
              - Build prompt for risk_judge_agent including the same inputs and the
                RiskAssessment.
              - Call risk_judge_agent.run(...).
              - Get RiskAssessmentJudgement.
              - Store both RiskAssessment and RiskAssessmentJudgement into Neo4j as nodes:
                  - (ra:RiskAssessment {incident_id,...})
                  - (rj:RiskAssessmentJudgement {incident_id,...})
                  - (i:Incident)-[:HAS_ASSESSMENT]->(ra)
                  - (ra)-[:HAS_JUDGEMENT]->(rj)
              - Print a human-readable summary:
                  - incident title and source
                  - risk_level
                  - referenced provisions
                  - judge score and issues.

          - Implement async main() that:
              - For now, uses a hardcoded INCIDENT_ID string for testing.
              - Later can be extended to parse sys.argv for --incident-id.
              - Calls assess_incident(INCIDENT_ID).

          - Add __main__ guard with asyncio.run(main()) and usage comments.

acceptance_criteria:
  - All new modules import cleanly (no syntax errors).
  - text_sanity.compute_text_stats + print_text_stats are used in at least
    run_preprocess_eu_ai_act.py and run_incident_preprocess_aiid.py to show
    text stats whenever a text is ingested.
  - run_incident_preprocess_aiid.py can be executed and produces AIIncident nodes
    in Neo4j for the hardcoded sample incidents.
  - run_assess_incident.py can be executed (with a test incident_id that exists
    in Neo4j) and prints:
      - a risk level,
      - a list of referenced provisions (even if simple),
      - a judge score and any issues.
  - No existing legal preprocessing scripts (for EU AI Act) or financial transcript
    scripts are broken by these additions.
