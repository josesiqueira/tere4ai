version: 1
name: eu-ai-act-preprocess-chunk-safety
description: >
  Fix the remaining context_length_exceeded error in the EU AI Act structural
  preprocessor by (1) adding a per-chunk size guard, (2) recursively splitting
  oversized chapter chunks into smaller subchunks by section/article groups,
  (3) reducing max_tokens for structural extraction, (4) using gpt-4o-mini for
  this step to save cost, and (5) logging chunk sizes for debugging.

context:
  project_root: trustworthy_project_v0
  existing_files:
    - .env
    - eu_ai_act.txt
    - config/neo4j_config.py
    - models/legal_structure.py
    - models/legal_preprocess.py
    - models/legal_chunks.py
    - eu_ai_act_splitter.py
    - agent_preprocess_eu_ai_act.py
    - ingest_preprocessed_legal.py
    - run_preprocess_eu_ai_act.py
  data_files:
    - path: eu_ai_act.txt
      url: /mnt/data/eu_ai_act.txt
      description: >
        Full plain-text of Regulation (EU) 2024/1689 – the EU AI Act. Used for
        structural preprocessing and for sizing/splitting chunks.

goals:
  - Ensure no single LLM call for structural preprocessing exceeds the model’s
    context window (128k tokens) even for very large chapters like CHAPTER III.
  - Introduce a robust, reusable mechanism for:
      - approximating token count per chunk,
      - enforcing a per-chunk input size limit,
      - further splitting oversized chapter chunks into smaller subchunks
        (by SECTION headings or article groups).
  - Reduce max_tokens for the structural agent to a sensible value to avoid
    over-reserving completion tokens and to reduce cost.
  - Use gpt-4o-mini for the structural agent to keep costs lower.
  - Log chunk sizes and approximate token counts to make debugging and
    future tuning easier.
  - Preserve the existing Pydantic models and ingestion flow; only adjust
    the chunking + agent call logic.

non_goals:
  - No changes to the semantic/risk assessment agents.
  - No changes to Neo4j schema or ingestion logic besides adapting to possibly
    more/finer-grained chapter chunks.
  - No attempt to perfectly estimate tokens; a simple heuristic is fine.

tasks:

  - id: approx-token-counter
    description: >
      Add a simple approximate token counter and a per-chunk input size limit
      to run_preprocess_eu_ai_act.py.
    changes:
      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Provide an approx_token_count(text) helper and constants for input
          limits; use them to decide when to further split chunks.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - Near the top of the file, after imports, add:

            # Very rough heuristic: ~4 characters per token for English legal text
            def approx_token_count(text: str) -> int:
                """
                Approximate the number of tokens in a text.

                This does NOT need to be exact, just conservative enough to
                detect extremely large chunks that risk hitting context limits.
                """
                if not text:
                    return 0
                return max(1, len(text) // 4)

            # Max tokens we allow for the *input* (messages) for a single call.
            # The model has a 128k total window (input + output); we keep a
            # healthy margin for instructions and completion.
            MAX_INPUT_TOKENS_PER_CHUNK = 80_000

          - When iterating over chunks (before calling the agent), log size info:

            print(
                f"[chunk {idx}/{total}] {chunk.kind} {chunk.label}: "
                f"{len(chunk.text)} chars (~{approx_token_count(chunk.text)} tokens)"
            )

          - Do NOT yet fail on large chunks here; that will be handled via a
            splitting function in eu_ai_act_splitter.py and the expanded
            chunk-preparation logic in this file (see other tasks).

  - id: split-large-chapter-chunk-helper
    description: >
      Implement a helper in eu_ai_act_splitter.py to further split oversized
      chapter chunks into smaller subchunks by SECTION headings or article
      groups.
    changes:
      - path: eu_ai_act_splitter.py
        purpose: >
          Add split_large_chapter_chunk(TextChunk) that takes a chapter chunk
          and returns multiple smaller TextChunk subchunks when needed.
        spec: |
          In eu_ai_act_splitter.py:

          - Ensure imports include:
              - import re
              - from typing import List
              - from models.legal_chunks import TextChunk, ChunkKind

          - Add a new helper function:

            def split_large_chapter_chunk(chunk: TextChunk, max_input_tokens: int = 80_000) -> List[TextChunk]:
                """
                If a chapter chunk is too large for a single LLM call, split it
                into smaller subchunks.

                Strategy:
                  1. Prefer splitting by SECTION headings within the chapter.
                     For example lines starting with 'SECTION 1', 'SECTION 2', etc.
                  2. If no SECTION headings are found, fall back to splitting
                     by 'Article <number>' groups (e.g. 5 articles per subchunk).

                The function assumes chunk.kind == 'chapter'.

                Returns:
                  - If the chunk is already small enough, returns [chunk].
                  - Otherwise, returns a list of smaller TextChunk objects with
                    kind='chapter' and more specific labels indicating the
                    section or article range.
                """
                assert chunk.kind == "chapter", "split_large_chapter_chunk is only for chapter chunks"

                text = chunk.text

                def _make_chunk(label_suffix: str, subtext: str) -> TextChunk:
                    return TextChunk(
                        kind="chapter",
                        label=f"{chunk.label} {label_suffix}".strip(),
                        text=subtext.strip(),
                    )

                # First, detect SECTION headings
                section_pattern = re.compile(r"^SECTION\s+\d+.*$", re.MULTILINE)
                sections = list(section_pattern.finditer(text))

                subchunks: List[TextChunk] = []

                if sections:
                    # Split by SECTION boundaries
                    for i, m in enumerate(sections):
                        start = m.start()
                        end = sections[i + 1].start() if i + 1 < len(sections) else len(text)
                        section_text = text[start:end]
                        section_heading = m.group(0).strip()
                        subchunks.append(_make_chunk(f"- {section_heading}", section_text))
                else:
                    # Fallback: split by Article headings in groups (e.g. 5 per subchunk)
                    article_pattern = re.compile(r"^Article\s+\d+.*$", re.MULTILINE)
                    articles = list(article_pattern.finditer(text))

                    if not articles:
                        # No obvious split points; return original chunk
                        return [chunk]

                    GROUP_SIZE = 5  # articles per subchunk
                    for i in range(0, len(articles), GROUP_SIZE):
                        group = articles[i : i + GROUP_SIZE]
                        start = group[0].start()
                        end = articles[i + GROUP_SIZE].start() if i + GROUP_SIZE < len(articles) else len(text)

                        subtext = text[start:end]
                        first_heading = group[0].group(0).strip()
                        last_heading = group[-1].group(0).strip()
                        label_suffix = f"(Articles {first_heading} - {last_heading})"
                        subchunks.append(_make_chunk(label_suffix, subtext))

                # If even the subchunks are too big, we keep them anyway because
                # we have already made them substantially smaller. The calling
                # code can still check approx_token_count if needed.

                # If somehow no subchunks were created, fall back to original chunk.
                return subchunks or [chunk]

  - id: expand-chunk-preparation
    description: >
      Expand the chunk preparation logic in run_preprocess_eu_ai_act.py to apply
      split_large_chapter_chunk() to oversized chapter chunks before calling
      the agent, so that no individual chunk exceeds the input budget.
    changes:
      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Replace the previous simple chunk list with a second-stage
          refinement: any chapter chunk whose approx_token_count exceeds the
          MAX_INPUT_TOKENS_PER_CHUNK threshold is further split into subchunks.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - Ensure you import the new helper from the splitter:

            from eu_ai_act_splitter import split_eu_ai_act, split_large_chapter_chunk

          - After you call split_eu_ai_act(raw_text) and before you start
            processing chunks, expand the base chunks into a refined list:

            base_chunks = split_eu_ai_act(raw_text)
            if not base_chunks:
                raise ValueError("Splitter returned no chunks for EU AI Act text.")

            refined_chunks = []
            for chunk in base_chunks:
                est_tokens = approx_token_count(chunk.text)
                if chunk.kind == "chapter" and est_tokens > MAX_INPUT_TOKENS_PER_CHUNK:
                    print(
                        f"Chunk '{chunk.label}' is too large (~{est_tokens} tokens); "
                        f"splitting into smaller chapter subchunks..."
                    )
                    subchunks = split_large_chapter_chunk(chunk, max_input_tokens=MAX_INPUT_TOKENS_PER_CHUNK)
                    refined_chunks.extend(subchunks)
                else:
                    refined_chunks.append(chunk)

            chunks = refined_chunks
            total = len(chunks)
            print(f"Using {total} refined chunks (recitals/chapters/annexes).")

          - When iterating over chunks for processing (where you previously
            used `chunks` and `total`), make sure you use this updated `chunks`
            variable.

          - Keep the logging line from the previous task to show char/token counts
            per chunk when processing starts.

  - id: lower-max-tokens-and-model
    description: >
      Reduce max_tokens for preprocess_legal_chunk_agent and use gpt-4o-mini
      for structural extraction to lower the risk of hitting the context limit
      and to reduce cost.
    changes:
      - path: agent_preprocess_eu_ai_act.py
        purpose: >
          Ensure the structural agent uses gpt-4o-mini and a modest max_tokens
          (e.g. 2048 or 4096), removing previous max_tokens=16384 hacks.
        spec: |
          In agent_preprocess_eu_ai_act.py:

          - Make sure the model is configured as gpt-4o-mini (not gpt-4o) for
            the structural chunk agent:

            from pydantic_ai.models.openai import OpenAIChatModel

            model = OpenAIChatModel(
                model_name="gpt-4o-mini",
            )

          - Wherever preprocess_legal_chunk_agent.run(...) is called (in
            run_preprocess_eu_ai_act.py), ensure that we do NOT pass
            max_tokens=16384. Instead, use a smaller, more sensible value:

            result = await preprocess_legal_chunk_agent.run(
                prompt,
                deps=deps,
                model_settings={"max_tokens": 2048},  # or 4096 if you prefer
            )

          - If there are any leftover comments about “fixing CHAPTER III token
            limit” via large max_tokens, remove or update them to reflect the
            new strategy: we now prevent oversized chunks and keep completion
            limits small for cost and safety.

  - id: merge-partial-chapter-results
    description: >
      Ensure that if a chapter has been split into multiple subchunks, the
      final PreprocessedLegalDocument still has a single Chapter object per
      chapter number, with all its articles/sections merged.
    changes:
      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Adjust the aggregation logic after chunk processing so that multiple
          ChunkResult.chapter objects for the same chapter (e.g. CHAPTER III
          sections or article ranges) are merged into one.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - When merging ChunkResult outputs into the final
            PreprocessedLegalDocument, do NOT simply append chapters blindly.

          - Instead, maintain a dict keyed by chapter number (or a combination
            of number + title) and merge articles/sections:

            from collections import OrderedDict
            from models.legal_structure import Chapter

            chapter_map: "OrderedDict[str, Chapter]" = OrderedDict()

            # Inside the chunk-processing loop, when you have chunk_result:
            if chunk_result.kind == "chapter" and chunk_result.chapter is not None:
                ch = chunk_result.chapter
                key = ch.number  # or f"{ch.number}:{ch.title}"

                if key not in chapter_map:
                    # First time we see this chapter; store it.
                    chapter_map[key] = ch
                else:
                    # Merge into existing Chapter:
                    existing = chapter_map[key]
                    # Merge sections:
                    existing.sections.extend(ch.sections)
                    # Merge articles:
                    existing.articles.extend(ch.articles)

          - After processing all chunks, set preprocessed.chapters from the
            values of chapter_map in insertion order:

            preprocessed.chapters = list(chapter_map.values())

          - Make sure recitals and annexes are still appended as before:
              preprocessed.recitals.extend(chunk_result.recitals) for recitals
              preprocessed.annexes.append(chunk_result.annex) for annexes.

  - id: chunk-size-logging
    description: >
      Improve logging around chunk sizes and processing to make diagnosis of
      future issues easier.
    changes:
      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Add clear logging for each chunk’s character length and approximate
          token count before calling the agent; log when chunks are further
          split; keep the existing progress lines.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - Ensure that before submitting a chunk to preprocess_legal_chunk_agent,
            you print something like:

            est_tokens = approx_token_count(chunk.text)
            print(
                f"[chunk {idx}/{total}] Processing {chunk.kind} {chunk.label} "
                f"({len(chunk.text)} chars, ~{est_tokens} tokens)..."
            )

          - When a large chapter chunk is split by split_large_chapter_chunk,
            log that event clearly (as described in the expand-chunk-preparation
            task).

          - Keep the existing summary lines (e.g. “Extracted N recitals”,
            “Extracted N articles”, etc.) after each chunk finishes.

acceptance_criteria:
  - Running `python3 run_preprocess_eu_ai_act.py` no longer throws a
    context_length_exceeded error, even for CHAPTER III or any oversized
    chapter chunk.
  - The script logs the character length and approximate token count for each
    chunk before processing.
  - If a chapter chunk is very large, the logs show that it is being further
    split and that multiple smaller subchunks are processed instead.
  - The final PreprocessedLegalDocument still contains:
      - correct counts of recitals,
      - a single Chapter object per chapter number with all its articles/sections,
      - all annexes.
  - The structural agent uses gpt-4o-mini and a modest max_tokens (2048/4096),
    with no remaining references to 16384 for this agent.
  - All existing ingestion logic into Neo4j continues to work without changes
    to schema; only the upstream preprocessing logic is altered.
