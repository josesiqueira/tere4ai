version: 1
name: eu-ai-act-structural-preprocessor-chunked
description: >
  Refactor the EU AI Act structural preprocessing to use a chunked LLM agent
  (per recitals/chapter/annex) instead of a single giant call, avoiding context
  limits while keeping the same trustworthy, Pydantic-based architecture.

context:
  project_root: trustworthy_project_v0
  existing_files:
    - .env
    - eu_ai_act.txt
    - config/neo4j_config.py
    - models/legal_structure.py
    - models/legal_preprocess.py
    - agent_preprocess_eu_ai_act.py
    - ingest_preprocessed_legal.py
    - run_preprocess_eu_ai_act.py
  notes:
    - The previous implementation called preprocess_legal_agent once with the
      entire eu_ai_act.txt and hit the 128k token limit.
    - We want to KEEP an LLM agent doing structural extraction, but operate on
      smaller, semantically meaningful chunks (recitals, one chapter, one annex).
    - The overall shape of the pipeline must remain:
        - LLM agent → Pydantic objects ONLY
        - Neo4j ingestion → deterministic, no LLMs
    - Existing financial transcript preprocessing code must not be broken.

  data_files:
    - path: eu_ai_act.txt
      url: /mnt/data/eu_ai_act.txt
      description: >
        Full plain-text of Regulation (EU) 2024/1689 – the Artificial
        Intelligence Act ("EU AI Act"), used as input to the splitter.

goals:
  - Introduce a deterministic splitter that breaks eu_ai_act.txt into chunks:
      - one chunk for all recitals,
      - one chunk per chapter,
      - one chunk per annex.
  - Introduce a chunk-level LLM agent that, given one chunk, returns the
    appropriate Pydantic structure (recitals list, one chapter, or one annex).
  - Update run_preprocess_eu_ai_act.py to:
      - loop over chunks,
      - call the chunk-level agent for each,
      - merge results into a single PreprocessedLegalDocument,
      - print clear, real progress like [3/15] chunks processed.
  - Keep ingest_preprocessed_legal.py unchanged (still ingests a complete
    PreprocessedLegalDocument).

non_goals:
  - No semantic classification yet (no risk levels, obligations, actors, HLEG).
  - Do not remove the existing preprocess_legal_agent if present; just stop
    using it in the runner for now.
  - No changes to the existing financial transcript pipeline.

tasks:

  - id: models-legal-chunkresult
    description: >
      Add a chunk-level result model so the agent can operate on one chunk at a
      time and still produce structured, Pydantic output.
    changes:
      - path: models/legal_chunks.py
        purpose: >
          Define ChunkKind, TextChunk (for splitter use), and ChunkResult to
          represent the output of processing a single chunk.
        spec: |
          Create a new file models/legal_chunks.py with:

          - Imports:
              - from __future__ import annotations
              - from dataclasses import dataclass
              - from typing import List, Literal, Optional
              - from pydantic import BaseModel, Field
              - from models.legal_structure import Recital, Chapter, Annex

          - Define a literal type for chunk kinds:

            ChunkKind = Literal["recitals", "chapter", "annex"]

          - Define a small dataclass used only by the splitter (no Pydantic needed):

            @dataclass
            class TextChunk:
                """
                A contiguous piece of the EU AI Act text corresponding to either:
                  - all recitals,
                  - a single chapter,
                  - a single annex.

                This is produced deterministically by the splitter and consumed
                by the runner, which then calls the LLM agent on each chunk.
                """
                kind: ChunkKind
                label: str        # e.g. "RECITALS", "CHAPTER I", "ANNEX III"
                text: str         # raw text for this chunk

          - Define a Pydantic model for the agent's per-chunk output:

            class ChunkResult(BaseModel):
                """
                Result of preprocessing a single text chunk.

                Exactly one of the fields (recitals, chapter, annex) is expected
                to be non-empty / non-None based on 'kind'.
                """
                document_id: str = Field(
                    description="Document identifier, echoed from deps (e.g. 'eu_ai_act_2024')."
                )
                kind: ChunkKind = Field(
                    description="What kind of chunk was parsed: recitals, chapter, or annex."
                )
                label: str = Field(
                    description="Label of this chunk (e.g. 'RECITALS', 'CHAPTER I', 'ANNEX III')."
                )

                # Only for kind == "recitals"
                recitals: List[Recital] = Field(
                    default_factory=list,
                    description="List of recitals parsed from this chunk (for kind='recitals')."
                )

                # Only for kind == "chapter"
                chapter: Optional[Chapter] = Field(
                    default=None,
                    description="Single Chapter parsed from this chunk (for kind='chapter')."
                )

                # Only for kind == "annex"
                annex: Optional[Annex] = Field(
                    default=None,
                    description="Single Annex parsed from this chunk (for kind='annex')."
                )

  - id: splitter-eu-ai-act
    description: >
      Add a deterministic splitter that breaks eu_ai_act.txt into semantically
      meaningful chunks (recitals, chapters, annexes) without using any LLM.
    changes:
      - path: eu_ai_act_splitter.py
        purpose: >
          Provide a function split_eu_ai_act(raw_text: str) -> list[TextChunk]
          that finds recitals, chapters, and annexes by regex/string patterns.
        spec: |
          Create eu_ai_act_splitter.py at the project root with:

          - Imports:
              - import re
              - from typing import List
              - from models.legal_chunks import TextChunk, ChunkKind

          - Implement:

            def split_eu_ai_act(raw_text: str) -> List[TextChunk]:
                """
                Deterministically split the EU AI Act plain text into chunks.

                The function returns a list of TextChunk objects in the order:
                  1. One 'recitals' chunk containing the entire 'Whereas' section.
                  2. One 'chapter' chunk per chapter (CHAPTER I, II, ...).
                  3. One 'annex' chunk per annex (ANNEX I, II, ...).

                It uses simple regex patterns on headings and does NOT call any LLMs.

                Assumptions (based on typical EU regulation layout):
                  - Recitals are introduced after a 'Whereas:' heading and before
                    the first 'CHAPTER ' heading.
                  - Chapters are marked by lines starting with 'CHAPTER ' followed
                    by a Roman numeral and a title.
                  - Annexes are marked by lines starting with 'ANNEX ' followed
                    by a Roman numeral and a title.

                If some parts cannot be cleanly split, include them in the nearest
                sensible chunk and document the decision in comments; do NOT drop text.
                """

                # 1. Identify recitals block (everything between 'Whereas' and first 'CHAPTER ').
                #    Use a conservative regex search for the first 'CHAPTER ' heading.
                # 2. Extract chapters as chunks: for each 'CHAPTER <ROMAN>' heading,
                #    take text until the next chapter or the first annex.
                # 3. Extract annexes as chunks: for each 'ANNEX <ROMAN>' heading,
                #    take text until the next annex or end of document.

                # Provide a clear, commented implementation with regex like:
                #   chapter_pattern = re.compile(r"^CHAPTER\\s+[IVXLC]+", re.MULTILINE)
                #   annex_pattern = re.compile(r"^ANNEX\\s+[IVXLC]+", re.MULTILINE)
                # and careful slicing using the match positions.

                # At the end, return a list[TextChunk].

                ...

          The implementation should:
            - NEVER call any LLM.
            - NOT drop any text; any part that doesn't match a heading should be
              included in some chunk (e.g. attached to the preceding chapter).
            - Prefer clarity over cleverness; add comments explaining each step.

  - id: agent-preprocess-legal-chunk
    description: >
      Add a chunk-level LLM agent that, given one chunk (recitals, one chapter,
      or one annex), outputs a ChunkResult Pydantic object.
    changes:
      - path: agent_preprocess_eu_ai_act.py
        purpose: >
          Extend this module by adding preprocess_legal_chunk_agent (do NOT
          remove existing preprocess_legal_agent, just stop using it in the runner).
        spec: |
          In agent_preprocess_eu_ai_act.py:

          - Ensure imports include:
              - from models.legal_chunks import ChunkResult
              - from models.legal_preprocess import LegalPreprocessDeps
              - from pydantic_ai import Agent
              - from pydantic_ai.models.openai import OpenAIChatModel

          - Reuse the existing model = OpenAIChatModel(...) instance if present.

          - Add a new agent:

            preprocess_legal_chunk_agent = Agent[LegalPreprocessDeps, ChunkResult](
                model,
                deps_type=LegalPreprocessDeps,
                output_type=ChunkResult,
                instructions=\"\"\"You are a structural parser for EU regulations, operating on ONE chunk at a time.

            You will receive:
              - Deterministic metadata (deps):
                  - document_id
                  - source_file
                  - jurisdiction
                  - instrument_type
              - A prompt describing:
                  - what kind of chunk this is: 'recitals', 'chapter', or 'annex'
                  - the label for the chunk (e.g. 'RECITALS', 'CHAPTER I', 'ANNEX III')
                  - the full raw text of this chunk only

            Your task is to output a ChunkResult object with:
              - document_id: echoed EXACTLY from deps
              - kind: one of 'recitals', 'chapter', 'annex' as specified in the prompt
              - label: echoed from the chunk label in the prompt
              - recitals: filled ONLY if kind == 'recitals'
              - chapter: filled ONLY if kind == 'chapter'
              - annex: filled ONLY if kind == 'annex'

            CRITICAL RULES (TRUSTWORTHINESS):

            1. NO HALLUCINATED STRUCTURE
               - Do NOT invent recitals, chapters, or annexes that are not clearly in THIS CHUNK.
               - Work ONLY within the boundaries of the provided text.
               - If you are unsure about a boundary, keep the text attached to the nearest
                 sensible element instead of guessing new elements.

            2. CHUNK TYPES

               For kind = 'recitals':
                 - Parse numbered recitals like '(1)', '(2)', '(3)', ...
                 - For each Recital, set:
                     number: integer from the '(n)' marker
                     text: all text after that marker up to the next '(n+1)' or end of recitals.

               For kind = 'chapter':
                 - The chunk contains exactly ONE chapter (e.g. 'CHAPTER II - TITLE').
                 - Extract:
                     Chapter.number: Roman numeral (e.g. 'II').
                     Chapter.title: the chapter title text.
                     Chapter.articles: all articles within this chapter.
                 - For each Article:
                     - number from 'Article N'
                     - title from the line after 'Article N'
                     - paragraphs: numbered '1.', '2.', '3.' etc.
                     - points: lettered '(a)', '(b)', '(c)' within paragraphs.

               For kind = 'annex':
                 - The chunk contains exactly ONE annex (e.g. 'ANNEX III - TITLE').
                 - Extract:
                     Annex.number: Roman numeral (e.g. 'III').
                     Annex.title: annex title line.
                     Annex.raw_text: everything after the heading as a single string.

            3. METADATA HANDLING
               - Echo document_id exactly from deps.
               - Echo the chunk label exactly as provided in the prompt.
               - Do NOT infer or change jurisdiction or instrument_type.

            4. TEXT HANDLING
               - Preserve original wording and ordering.
               - Do NOT summarise or rephrase; this is structural extraction only.
               - If some text in the chunk does not fit cleanly into your structure,
                 include it in the nearest Recital/Paragraph/Point/Annex rather than
                 dropping it.

            5. OUTPUT FORMAT
               - Return a single ChunkResult instance.
               - For kind='recitals': chapter and annex MUST be null.
               - For kind='chapter': recitals must be empty, annex must be null.
               - For kind='annex': recitals must be empty, chapter must be null.

            Your job is to convert one chunk of regulation text into structured,
            lossless, conservative Pydantic objects with NO semantic interpretation.
            \"\"\",
            )

          - Do NOT modify any other existing agents; we simply introduce this
            new agent for per-chunk processing.


  - id: runner-eu-ai-act-chunked
    description: >
      Update run_preprocess_eu_ai_act.py to use the splitter and chunk-level
      agent, build a full PreprocessedLegalDocument in Python, and then ingest.
    changes:
      - path: run_preprocess_eu_ai_act.py
        purpose: >
          Replace the single giant LLM call with a loop over chunks, showing
          real progress and merging ChunkResult objects into a single
          PreprocessedLegalDocument.
        spec: |
          In run_preprocess_eu_ai_act.py:

          - Ensure imports include:
              - import asyncio
              - from pathlib import Path
              - from contextlib import suppress
              - from models.legal_structure import PreprocessedLegalDocument, Chapter, Annex
              - from models.legal_preprocess import LegalPreprocessDeps
              - from models.legal_chunks import TextChunk, ChunkKind, ChunkResult
              - from agent_preprocess_eu_ai_act import preprocess_legal_chunk_agent
              - from eu_ai_act_splitter import split_eu_ai_act
              - from ingest_preprocessed_legal import ingest_preprocessed_legal_document

          - Implement a helper:

            def read_eu_ai_act_text(file_path: Path) -> str:
                """
                Read the full EU AI Act text from a plain-text file.

                Args:
                    file_path: Path to 'eu_ai_act.txt'

                Returns:
                    The full text as a string.

                Raises:
                    FileNotFoundError: if the file is missing
                    ValueError: if the file is empty
                """
                if not file_path.exists():
                    raise FileNotFoundError(f"File not found: {file_path}")
                text = file_path.read_text(encoding="utf-8")
                if not text.strip():
                    raise ValueError(f"Empty text in {file_path}")
                return text

          - Implement an async function to process all chunks:

            async def preprocess_eu_ai_act_by_chunks() -> PreprocessedLegalDocument:
                DOCUMENT_ID = "eu_ai_act_2024"
                SOURCE_FILE = "eu_ai_act.txt"
                OFFICIAL_TITLE = (
                    "Regulation (EU) 2024/1689 of the European Parliament and of "
                    "the Council ... (Artificial Intelligence Act)"
                )

                file_path = Path(SOURCE_FILE)
                raw_text = read_eu_ai_act_text(file_path)

                deps = LegalPreprocessDeps(
                    document_id=DOCUMENT_ID,
                    source_file=SOURCE_FILE,
                    jurisdiction="EU",
                    instrument_type="Regulation",
                )

                # Split into deterministic chunks
                chunks = split_eu_ai_act(raw_text)
                if not chunks:
                    raise ValueError("Splitter returned no chunks for EU AI Act text.")

                total = len(chunks)
                print(f"Found {total} chunks (recitals/chapters/annexes).")

                # Initialize empty PreprocessedLegalDocument
                preprocessed = PreprocessedLegalDocument(
                    document_id=DOCUMENT_ID,
                    official_title=OFFICIAL_TITLE,
                    short_title="EU AI Act",
                    year=2024,
                    recitals=[],
                    chapters=[],
                    annexes=[],
                )

                for idx, chunk in enumerate(chunks, start=1):
                    print(f"\n[chunk {idx}/{total}] Processing {chunk.kind} {chunk.label}...")

                    # Build a chunk-specific prompt
                    prompt = (
                        f"You are parsing a {chunk.kind} chunk of the EU AI Act.\n"
                        f"Chunk label: {chunk.label}\n\n"
                        f"Below is the full text of this chunk:\n\n"
                        f"{chunk.text}"
                    )

                    result = await preprocess_legal_chunk_agent.run(prompt, deps=deps)
                    chunk_result: ChunkResult = result.output

                    # Merge into final PreprocessedLegalDocument
                    if chunk_result.kind == "recitals":
                        preprocessed.recitals.extend(chunk_result.recitals)
                    elif chunk_result.kind == "chapter":
                        if chunk_result.chapter is not None:
                            preprocessed.chapters.append(chunk_result.chapter)
                    elif chunk_result.kind == "annex":
                        if chunk_result.annex is not None:
                            preprocessed.annexes.append(chunk_result.annex)

                    print(f"✓ Finished {chunk.kind} {chunk.label} ({idx}/{total})")

                return preprocessed

          - Update main() to use this new function and then ingest:

            async def main():
                print("Prerequisites:")
                print("  ✓ OPENAI_API_KEY in .env file")
                print("  ✓ Neo4j running on localhost:7687")
                print("  ✓ eu_ai_act.txt in project root\n")

                print("======================================================================")
                print("EU AI ACT - STRUCTURAL PREPROCESSING (CHUNKED)")
                print("======================================================================\n")
                print("This script extracts the physical structure of the EU AI Act")
                print("(recitals, chapters, articles, annexes) and ingests it into Neo4j.\n")

                print("[1/2] Running structural preprocessing agent (chunked)...")

                try:
                    preprocessed = await preprocess_eu_ai_act_by_chunks()
                except Exception as e:
                    print(f"❌ Preprocessing failed: {e}")
                    return

                # Compute a simple summary
                total_articles = sum(
                    len(ch.articles) + sum(len(sec.articles) for sec in ch.sections)
                    for ch in preprocessed.chapters
                )

                print("\n✓ Structural preprocessing completed.")
                print(f"  - Recitals:  {len(preprocessed.recitals)}")
                print(f"  - Chapters:  {len(preprocessed.chapters)}")
                print(f"  - Articles:  {total_articles}")
                print(f"  - Annexes:   {len(preprocessed.annexes)}")

                print("\n[2/2] Ingesting into Neo4j...")
                try:
                    ingest_preprocessed_legal_document(preprocessed)
                    print("✓ Ingestion complete.")
                except Exception as e:
                    print(f"❌ Error during ingestion: {e}")

            if __name__ == "__main__":
                asyncio.run(main())

          - Ensure no remaining references in this file to the old
            preprocess_legal_agent single-shot call.

acceptance_criteria:
  - Running `python3 run_preprocess_eu_ai_act.py` no longer results in a
    context_length_exceeded error from the model.
  - The script prints progress like:
      - "Found N chunks (recitals/chapters/annexes)."
      - "[chunk 1/N] Processing chapter CHAPTER I..."
      - "✓ Finished chapter CHAPTER I (1/N)"
  - At the end of preprocessing it prints a summary with counts of recitals,
    chapters, articles, and annexes.
  - ingest_preprocessed_legal_document(preprocessed) still runs and writes nodes
    to Neo4j, using the existing schema.
  - Existing financial transcript preprocessing code remains untouched and
    functional.
